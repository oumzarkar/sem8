What is linear regression
How deep neural networks work
Boston housing is example of implementation of deep neural network
Steps performed in assignment-
Data preprocessing
Model architecture
Model training---weights and biases adjusted based on predicted and actual 
Model evaluation---mse mae
Boston housing dataset----
---506 houses
---CRIM-crime rate per capita
----ZN-propertion of residential land zoned
----INDUS-proportion of non retail buisenness 
--CHAS-RIVER
--NOX-nitric oxide conc
---RM-avg no. of room cost
--AGE—proportion of owner occupied land
---DIS—dist. Of 5 weigted bosten employement centers
--TAX—tax rate
---PTRATIO-pupil teacher ratio
---B- black people
---LSTAT- lower status of population

Output--- it is median value of owner occupied homes
Explain concept of standardization
Why you split data in train test
Application of deep neural network


Letter recognistion
Multiclass classification using deep neural networks---concept used
Attributes-17
Instances/dataset size-20k
Input image-image preprocessing-text detection-text recognistionrestructuring-output text
What is multiclass classification
What is binary classification
We are using label encoder to encode letters into numeric labels
Sequential()—linear stack of labels

•	The first layer added is a Dense layer with 128 units and ReLU activation function. The input_shape parameter is set to (16,) indicating that the input data has 16 features.
•	The second layer is another Dense layer with 64 units and ReLU activation.
•	The final output layer is a Dense layer with 26 units (corresponding to the 26 classes of letters) and softmax activation, which outputs probabilities for each class.
Assignment three
Plant desease using cnn
Mnist fation dataset
Cnn layers-
1)input layer
2)feature extraction layer
3) classification layer---accepts 3d input hight width and color(rgb)
In 2) we have 2 more
2.1 convolutional layer
2.2 pooling layer
What is cnn
What is leNet and alexNet

Assignment four
Using google stock prices dataset and design a time series analysis prediction using rnn
What is rnn
Rnn vs cnn
Varients of rnn
Limitations of rnn

Miniproject
Human face recognistion
Haar Cascade classifier using opencv setup
We are using a pre tarained cascade classifier model
Model loading—cv2.cascadeclassifier
Preprocessing---capture frames---Cv2.video caspture
	          ----convert captured frames to greyscale
Dedect faces using facecascade.detectMultiscale
Post processing---draw refctangle using cv2.rectangle
Display output using cv2.imshow


HPC
Assignment one
What is bubble sort and merge sort
Openmp is a set of compiler directives it identifies parallel regions in block of code that my run in parallel
Pragma omp parallel—it creates threads equal to the no. of cores and then all the threads are simultaneously executed
What is scaling parallel merge sort

Assignment two
Parallel bfs and dfs
Advantage of parallel programing in bfs dfs
Assignment three
Parallel reduction using min,max,sum and avg in cuda
How cuda is usefull
What is parallel reduction operations
How parallelisum is achived in cuda
Exlain grid block and thread struction in relation with prallel reduction
Assignment four
Addistion of 2 vectors and matrix multi
Advantages of cuda for Addistion of 2 vectors and matrix multi
Steps for addition and multi-
1)Define size of vectors
2)allocate memory on host
3)initialize vectors
4)allocate memory on device
5)copy input vectors from host
6)launch kernal
7)copy result to host
8)free memony on device
9)free memonry on host

Linear Regression: Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the independent variables and the dependent variable. The goal of linear regression is to find the best-fitting line (or plane in higher dimensions) that minimizes the vertical distance between the observed data points and the predicted values generated by the model.
How Deep Neural Networks Work: Deep neural networks (DNNs) are a type of artificial neural network (ANN) that consists of multiple layers of interconnected nodes (neurons). Each layer performs a transformation on the input data, passing it through a series of nonlinear activation functions to learn increasingly abstract features from the data. The layers are typically organized into an input layer, one or more hidden layers, and an output layer. During training, DNNs adjust their internal parameters (weights and biases) using optimization algorithms such as gradient descent to minimize the difference between the predicted output and the true output, as measured by a loss function.
Boston Housing Example: The Boston Housing dataset is an example of using a deep neural network for regression tasks. In this example, the DNN is trained to predict the median value of owner-occupied homes based on various features such as crime rate, average number of rooms, pupil-teacher ratio, etc.
Steps Performed in Assignment:
1.	Data Preprocessing: This involves cleaning the data, handling missing values, and performing feature scaling or normalization.
2.	Model Architecture: Designing the architecture of the deep neural network, including the number of layers, types of activation functions, and number of neurons in each layer.
3.	Model Training: Adjusting the weights and biases of the neural network based on the predicted and actual values using optimization algorithms.
4.	Model Evaluation: Assessing the performance of the trained model using evaluation metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE).
Concept of Standardization: Standardization is a preprocessing technique used to transform the features of a dataset to have a mean of 0 and a standard deviation of 1. It helps in making the features comparable by bringing them to a common scale, which can improve the performance of machine learning models, especially those sensitive to the scale of input features.
Why Split Data into Train and Test: Splitting the data into training and testing sets allows us to assess the performance of the model on unseen data. The training set is used to train the model, while the testing set is used to evaluate its performance and generalization ability. This helps in detecting overfitting and ensures that the model can make accurate predictions on new, unseen data.
Application of Deep Neural Networks: Deep neural networks have various applications across domains such as image and speech recognition, natural language processing, recommendation systems, healthcare, finance, and more. They excel at learning complex patterns and representations from large volumes of data, making them suitable for tasks requiring high-dimensional input and output spaces.

what is Multiclass classification using deep neural networks
ChatGPT
ChatGPT
Multiclass classification using deep neural networks refers to a type of classification problem where the goal is to classify input data into one of three or more classes or categories. In other words, instead of just distinguishing between two classes (binary classification), multiclass classification involves distinguishing between multiple classes.
What is cnn What is leNet and alexNet 
ChatGPT
ChatGPT
A Convolutional Neural Network (CNN) is a type of deep neural network specifically designed for processing structured grid-like data, such as images. CNNs are particularly effective for tasks like image classification, object detection, and image segmentation. They are inspired by the organization and functionality of the visual cortex in animals, where neurons respond to stimuli in a restricted region of the visual field, known as the receptive field.
LeNet-5, developed by Yann LeCun et al. in the 1990s, is one of the pioneering CNN architectures. It was designed for handwritten digit recognition tasks and consists of convolutional layers followed by average pooling and fully connected layers. LeNet-5 played a significant role in popularizing CNNs and demonstrating their effectiveness in pattern recognition tasks.
AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, is a deeper and more complex CNN architecture that achieved breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. AlexNet consists of multiple convolutional layers, max-pooling layers, and fully connected layers. It introduced several innovations, such as the use of ReLU activation functions, local response normalization, and dropout regularization, which contributed to its superior performance.
What is rnn Rnn vs cnn Varients of rnn Limitations of rnn 
ChatGPT
ChatGPT
A Recurrent Neural Network (RNN) is a type of neural network architecture designed for processing sequential data, where the order of the data points matters. RNNs have connections between nodes that form directed cycles, allowing them to exhibit temporal dynamic behavior. They are widely used for tasks such as natural language processing (NLP), time series prediction, speech recognition, and handwriting recognition.
Key characteristics of RNNs include:
1.	Recurrent Connections: RNNs have connections that form cycles, allowing information to persist over time and influence future predictions.
2.	Hidden State: RNNs maintain a hidden state vector that summarizes information from previous time steps and serves as the memory of the network.
3.	Time Unfolding: RNNs can be unfolded over time, revealing a series of interconnected neural network layers, each processing input from a specific time step.
RNNs vs. CNNs:
•	RNNs are designed for sequential data processing, whereas CNNs are designed for grid-like data such as images.
•	RNNs maintain state across time steps, allowing them to capture temporal dependencies, while CNNs operate on fixed-size input and do not explicitly maintain state.
•	RNNs are well-suited for tasks like time series prediction and sequence generation, while CNNs excel at tasks like image classification and object detection.
Variants of RNNs:
1.	Long Short-Term Memory (LSTM): LSTM networks are a type of RNN architecture that addresses the vanishing gradient problem by introducing specialized memory cells and gating mechanisms. They are capable of learning long-term dependencies and have been widely used in various sequence modeling tasks.
2.	Gated Recurrent Unit (GRU): GRU networks are similar to LSTM networks but have a simpler architecture with fewer parameters. They also incorporate gating mechanisms to control the flow of information, enabling them to capture long-term dependencies in sequential data.
3.	Bidirectional RNNs: Bidirectional RNNs process input sequences in both forward and backward directions, allowing them to capture contextual information from both past and future time steps.
Limitations of RNNs:
1.	Vanishing Gradient Problem: RNNs are susceptible to the vanishing gradient problem, where gradients diminish as they propagate back through time, making it challenging to learn long-term dependencies.
2.	Difficulty in Capturing Long-Term Dependencies: Standard RNN architectures struggle to capture long-term dependencies in sequences, leading to limitations in tasks requiring long-range temporal understanding.
3.	Computational Complexity: RNNs can be computationally expensive to train, especially when processing long sequences or dealing with large parameter counts in models like LSTMs.


Assignment one:
1.	Bubble Sort and Merge Sort:
•	Bubble Sort: Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.
•	Merge Sort: Merge Sort is a divide-and-conquer algorithm that divides the unsorted list into sublists, recursively sorts the sublists, and then merges the sorted sublists to produce the final sorted list.
2.	OpenMP:
•	OpenMP (Open Multi-Processing) is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. It allows developers to parallelize code by adding compiler directives to indicate sections of code that can be executed concurrently.
•	#pragma omp parallel: This directive creates a team of threads, where each thread executes a copy of the parallel region code. The number of threads created is typically equal to the number of available CPU cores.
3.	Scaling Parallel Merge Sort:
•	Scaling parallel merge sort involves optimizing the parallel implementation of the merge sort algorithm to efficiently utilize resources as the problem size increases. This may include strategies such as load balancing, minimizing communication overhead, and maximizing parallelism.
Assignment two:
1.	Parallel BFS and DFS:
•	Parallel Breadth-First Search (BFS) and Depth-First Search (DFS) algorithms involve executing these graph traversal algorithms concurrently using multiple threads or processes.
•	BFS explores all neighbor nodes at the present depth prior to moving on to the nodes at the next depth level. DFS explores as far as possible along each branch before backtracking.
2.	Advantages of Parallel Programming in BFS and DFS:
•	Parallel execution can significantly speed up BFS and DFS algorithms, especially for large graphs, by leveraging multiple processing units simultaneously.
•	Parallel BFS and DFS can efficiently explore different branches of the graph concurrently, leading to faster exploration and traversal times.
Assignment three:
1.	Parallel Reduction using Min, Max, Sum, and Avg in CUDA:
•	Parallel reduction involves efficiently computing aggregate values (such as minimum, maximum, sum, or average) across large datasets using parallel processing techniques.
•	CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for GPU acceleration.
2.	Advantages of CUDA:
•	CUDA allows developers to harness the computational power of NVIDIA GPUs for general-purpose parallel computing tasks.
•	It provides a high-level programming interface for writing parallel algorithms that can exploit the massively parallel architecture of GPUs.
3.	Parallel Reduction Operations:
•	Parallel reduction operations involve reducing a large dataset to a smaller set of aggregated values using parallel processing techniques.
•	Examples include finding the minimum or maximum value in an array, computing the sum or average of array elements, etc.
4.	Parallelism in CUDA:
•	Parallelism in CUDA is achieved by launching multiple threads, organized into blocks and grids, to execute kernel functions concurrently on the GPU.
Assignment four:
1.	Addition of 2 Vectors and Matrix Multiplication:
•	These are fundamental linear algebra operations performed in parallel using CUDA for GPU acceleration.
•	The steps typically involve memory allocation, data transfer between the host (CPU) and device (GPU), kernel execution for parallel computation, and result retrieval.
2.	Advantages of CUDA for Vector Addition and Matrix Multiplication:
•	CUDA allows these operations to be parallelized and executed on GPU cores, which can lead to significant speedup compared to sequential CPU-based implementations.
•	GPUs are highly parallel architectures with many cores, making them well-suited for data-parallel tasks like vector addition and matrix multiplication.
